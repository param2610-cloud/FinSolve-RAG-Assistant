{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0233d78c",
   "metadata": {},
   "source": [
    "we will know about role of user at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71c99ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Parambrata Ghosh\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f7735",
   "metadata": {},
   "source": [
    "we will have the user list with roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1a418ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "database=[\n",
    "    {\n",
    "        \"id\":1,\n",
    "        \"name\": \"Parambrata Ghosh\",\n",
    "        \"role\": \"Manager\"\n",
    "    },\n",
    "    {\n",
    "        \"id\":2,\n",
    "        \"name\": \"Parthib Panja\",\n",
    "        \"role\": \"Engineer\"\n",
    "    },\n",
    "    {\n",
    "        \"id\":3,\n",
    "        \"name\": \"Shramana Show\",\n",
    "        \"role\":\"hr\"\n",
    "    },\n",
    "    {\n",
    "        \"id\":4,\n",
    "        \"name\":\"Jeet Mahapatra\",\n",
    "        \"role\":\"Marketing\"\n",
    "    },\n",
    "    {\n",
    "        \"id\":5,\n",
    "        \"name\":\"Nayan Samanta\",\n",
    "        \"role\":\"Employee\"\n",
    "    },\n",
    "    {\n",
    "        \"id\":6,\n",
    "        \"name\":\"Hiranmay Pore\",\n",
    "        \"role\":\"Finance\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7899ae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "role=None\n",
    "found_item=None\n",
    "for item in database:\n",
    "    if item['name']==name:\n",
    "        found_item=item\n",
    "        role=item['role']\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "776216da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jwt\n",
    "import dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed62e26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'name': 'Parambrata Ghosh',\n",
       " 'role': 'Manager',\n",
       " 'token': 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6MSwicm9sZSI6Ik1hbmFnZXIifQ.uz50pJw_C_ifStkI1i0xswGs9L3nltVjfZiWvmdEdOM'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()\n",
    "def generate_token(id):\n",
    "    payload= None\n",
    "    for item in database:\n",
    "        if item['id']==id:\n",
    "            payload={\n",
    "                \"id\":item['id'],\n",
    "                \"role\":item['role']\n",
    "            }\n",
    "            break\n",
    "    token = jwt.encode(payload, os.getenv(\"JWT_SECRET_KEY\"), algorithm='HS256')\n",
    "    return token\n",
    "token=generate_token(found_item['id'])\n",
    "found_item['token']=token\n",
    "found_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7a5ccf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Manager'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a685b31",
   "metadata": {},
   "source": [
    "make a token which will be valid for next 8 hours , with each request you have to pass this token for verification. The token will contained  id of user and role. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e80c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the marketing strategies for Q4?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e409c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE_PERMISSIONS = {\n",
    "    \"Manager\":[\"finance\",\"hr\",\"engineering\",\"marketing\",\"general\"],\n",
    "    \"Engineer\":[\"engineering\",\"general\"],\n",
    "    \"hr\":[\"hr\",\"general\"],\n",
    "    \"Marketing\":[\"marketing\",\"general\"],\n",
    "    \"Finance\":[\"finance\",\"general\"],\n",
    "    \"Employee\":[\"general\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a96c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37a9cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_with_metadata():\n",
    "    documents = []\n",
    "    dept_mapping = {\n",
    "        \"data/finance\":\"finance\",\n",
    "        \"data/hr\":\"hr\",\n",
    "        \"data/engineering\":\"engineering\",\n",
    "        \"data/marketing\":\"marketing\",\n",
    "        \"data/general\":\"general\"\n",
    "    }\n",
    "    for folder, dept in dept_mapping.items():\n",
    "        if os.path.exists(folder):\n",
    "            loader = DirectoryLoader(folder, glob=\"**/*.md\",show_progress=True)\n",
    "            docs = loader.load()\n",
    "            for doc in docs:\n",
    "                doc.metadata['department'] = dept\n",
    "                doc.metadata['source'] =doc.metadata.get('source','').replace(\"\\\\\"  , \"/\")\n",
    "                documents.append(doc)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0fe30c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5d7406",
   "metadata": {},
   "source": [
    "load hr data at first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc4e6490",
   "metadata": {},
   "outputs": [],
   "source": [
    "HR_DATA=None\n",
    "hr_csv_path = \"data/hr/hr_data.csv\"\n",
    "if os.path.exists(hr_csv_path):\n",
    "    HR_DATA= pd.read_csv(hr_csv_path)\n",
    "else:\n",
    "    HR_DATA= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "566ad5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f74aa9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vector_store():\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={\"device\": \"cpu\"}\n",
    "    )\n",
    "    persist_directory = \"./database/vector_store\"\n",
    "    documents = load_documents_with_metadata()\n",
    "    current_sources = {doc.metadata.get(\"source\") for doc in documents if doc.metadata.get(\"source\")}\n",
    "    \n",
    "    need_rebuild = not os.path.exists(persist_directory)\n",
    "    vector_store = None\n",
    "    \n",
    "    if not need_rebuild:\n",
    "        print(\"Vector store already exists. Loading from disk...\")\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=persist_directory,\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=\"company_docs\"\n",
    "        )\n",
    "        try:\n",
    "            existing = vector_store.get(include=[\"metadatas\"])\n",
    "            stored_sources = {\n",
    "                (meta or {}).get(\"source\")\n",
    "                for meta in existing.get(\"metadatas\", [])\n",
    "                if meta and meta.get(\"source\")\n",
    "            }\n",
    "        except Exception as exc:\n",
    "            print(f\"⚠️ Unable to inspect existing vector store ({exc}). Rebuilding...\")\n",
    "            stored_sources = set()\n",
    "        if current_sources and not current_sources.issubset(stored_sources):\n",
    "            print(\"Detected new or missing documents. Refreshing vector store...\")\n",
    "            need_rebuild = True\n",
    "    \n",
    "    if need_rebuild:\n",
    "        print(\"Creating new vector store...\")\n",
    "        from shutil import rmtree\n",
    "        rmtree(persist_directory, ignore_errors=True)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len\n",
    "        )\n",
    "        splits = text_splitter.split_documents(documents)\n",
    "        vector_store = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=persist_directory,\n",
    "            collection_name=\"company_docs\"\n",
    "        )\n",
    "        print(f\"Vector store created with {len(splits)} documents.\")\n",
    "    \n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f42d1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 17.92it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 17.92it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.65it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 31.42it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 31.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store already exists. Loading from disk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vector_store = initialize_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48abd582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f144097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator, Dict, Any, Tuple, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8f574bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_hr_data(question: str, user_role: str, processed_query: Dict) -> Tuple[str, pd.DataFrame]:\n",
    "    \"\"\"Handle HR-specific queries using spaCy-processed information\"\"\"\n",
    "    \n",
    "    if \"hr\" not in ROLE_PERMISSIONS.get(user_role, []):\n",
    "        return \"❌ Access Denied: You don't have permission to access HR data.\", None\n",
    "    \n",
    "    if HR_DATA is None:\n",
    "        return \"❌ HR data not found.\", None\n",
    "    \n",
    "    # Use processed query information\n",
    "    entities = processed_query[\"entities\"]\n",
    "    intent = processed_query[\"intent\"]\n",
    "    query_lower = processed_query[\"clean_query\"].lower()\n",
    "    \n",
    "    try:\n",
    "        result_df = None\n",
    "        \n",
    "        # Search by employee name (using spaCy entity extraction)\n",
    "        if entities[\"persons\"]:\n",
    "            person_name = entities[\"persons\"][0]\n",
    "            result_df = HR_DATA[HR_DATA['full_name'].str.contains(person_name, case=False, na=False)]\n",
    "            if not result_df.empty:\n",
    "                return f\"✅ Found employee: {person_name}\", result_df\n",
    "        \n",
    "        # Salary queries with aggregation detection\n",
    "        if \"salary\" in query_lower or \"payroll\" in query_lower or \"compensation\" in query_lower:\n",
    "            if intent[\"is_aggregation\"]:\n",
    "                # Calculate statistics\n",
    "                stats = {\n",
    "                    'Average Salary': [f\"₹{HR_DATA['salary'].mean():,.2f}\"],\n",
    "                    'Median Salary': [f\"₹{HR_DATA['salary'].median():,.2f}\"],\n",
    "                    'Min Salary': [f\"₹{HR_DATA['salary'].min():,.2f}\"],\n",
    "                    'Max Salary': [f\"₹{HR_DATA['salary'].max():,.2f}\"],\n",
    "                    'Total Employees': [len(HR_DATA)]\n",
    "                }\n",
    "                result_df = pd.DataFrame(stats)\n",
    "            elif \"highest\" in query_lower or \"top\" in query_lower:\n",
    "                # Extract number if specified\n",
    "                top_n = 10\n",
    "                if entities[\"numbers\"]:\n",
    "                    try:\n",
    "                        top_n = int(entities[\"numbers\"][0])\n",
    "                    except:\n",
    "                        pass\n",
    "                result_df = HR_DATA.nlargest(top_n, 'salary')[['employee_id', 'full_name', 'role', 'department', 'salary']]\n",
    "            elif \"lowest\" in query_lower or \"bottom\" in query_lower:\n",
    "                result_df = HR_DATA.nsmallest(10, 'salary')[['employee_id', 'full_name', 'role', 'department', 'salary']]\n",
    "            else:\n",
    "                result_df = HR_DATA[['employee_id', 'full_name', 'role', 'department', 'salary']]\n",
    "        \n",
    "        if result_df is not None and not result_df.empty:\n",
    "            return \"✅ HR Data Retrieved\", result_df.head(20)\n",
    "        \n",
    "        return \"⚠️ No matching HR data found\", None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"❌ Error processing HR query: {str(e)}\", None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e983dd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HR Department queries handler\n",
    "def query_hr_departments(query_lower: str, intent: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Handle department-specific HR queries\"\"\"\n",
    "    if intent[\"is_aggregation\"]:\n",
    "        dept_stats = HR_DATA.groupby('department').agg({\n",
    "            'employee_id': 'count',\n",
    "            'salary': ['mean', 'sum'],\n",
    "            'performance_rating': 'mean'\n",
    "        }).round(2)\n",
    "        dept_stats.columns = ['Employee Count', 'Avg Salary', 'Total Salary', 'Avg Performance']\n",
    "        return dept_stats.reset_index()\n",
    "    else:\n",
    "        return HR_DATA[['employee_id', 'full_name', 'department', 'role']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5753572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HR Performance queries handler\n",
    "def query_hr_performance(query_lower: str) -> pd.DataFrame:\n",
    "    \"\"\"Handle performance-related HR queries\"\"\"\n",
    "    if \"top\" in query_lower or \"highest\" in query_lower:\n",
    "        return HR_DATA.nlargest(10, 'performance_rating')[['employee_id', 'full_name', 'department', 'performance_rating', 'last_review_date']]\n",
    "    else:\n",
    "        return HR_DATA[['employee_id', 'full_name', 'department', 'performance_rating', 'last_review_date']].sort_values('performance_rating', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aba4ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HR Leave/Attendance queries handler\n",
    "def query_hr_attendance(query_lower: str, intent: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Handle leave and attendance queries\"\"\"\n",
    "    if intent[\"is_aggregation\"]:\n",
    "        avg_stats = {\n",
    "            'Avg Leave Balance': [HR_DATA['leave_balance'].mean()],\n",
    "            'Avg Leaves Taken': [HR_DATA['leaves_taken'].mean()],\n",
    "            'Avg Attendance %': [f\"{HR_DATA['attendance_pct'].mean():.2f}%\"]\n",
    "        }\n",
    "        return pd.DataFrame(avg_stats)\n",
    "    else:\n",
    "        return HR_DATA[['employee_id', 'full_name', 'leave_balance', 'leaves_taken', 'attendance_pct']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c7a73a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update main query_hr_data to use helper functions\n",
    "def query_hr_data_extended(question: str, user_role: str, processed_query: Dict) -> Tuple[str, pd.DataFrame]:\n",
    "    \"\"\"Extended HR query handler with all query types\"\"\"\n",
    "    \n",
    "    status, result_df = query_hr_data(question, user_role, processed_query)\n",
    "    \n",
    "    # If already found result, return it\n",
    "    if result_df is not None:\n",
    "        return status, result_df\n",
    "    \n",
    "    # Otherwise check other query types\n",
    "    query_lower = processed_query[\"clean_query\"].lower()\n",
    "    intent = processed_query[\"intent\"]\n",
    "    \n",
    "    try:\n",
    "        # Department queries\n",
    "        if \"department\" in query_lower:\n",
    "            return \"✅ HR Data Retrieved\", query_hr_departments(query_lower, intent)\n",
    "        \n",
    "        # Performance queries\n",
    "        elif \"performance\" in query_lower or \"rating\" in query_lower:\n",
    "            return \"✅ HR Data Retrieved\", query_hr_performance(query_lower)\n",
    "        \n",
    "        # Leave/Attendance queries\n",
    "        elif \"leave\" in query_lower or \"attendance\" in query_lower:\n",
    "            return \"✅ HR Data Retrieved\", query_hr_attendance(query_lower, intent)\n",
    "        \n",
    "        # Default summary\n",
    "        else:\n",
    "            summary = {\n",
    "                'Total Employees': [len(HR_DATA)],\n",
    "                'Average Salary': [f\"₹{HR_DATA['salary'].mean():,.2f}\"],\n",
    "                'Departments': [HR_DATA['department'].nunique()],\n",
    "                'Avg Performance': [f\"{HR_DATA['performance_rating'].mean():.2f}\"],\n",
    "                'Avg Attendance': [f\"{HR_DATA['attendance_pct'].mean():.2f}%\"]\n",
    "            }\n",
    "            return \"✅ HR Summary\", pd.DataFrame(summary)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"❌ Error: {str(e)}\", None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bb3b2aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e7b252f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1980cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPARTMENT_KEYWORDS = {\n",
    "    \"finance\": [\"finance\", \"financial\", \"revenue\", \"expense\", \"budget\", \"cost\", \"profit\", \"quarter\", \"q1\", \"q2\", \"q3\", \"q4\", \"quarterly\", \"annual\", \"payment\", \"invoice\"],\n",
    "    \"marketing\": [\"marketing\", \"campaign\", \"customer\", \"acquisition\", \"conversion\", \"roi\", \"ad\", \"advertisement\", \"brand\", \"social media\", \"engagement\"],\n",
    "    \"hr\": [\"employee\", \"hr\", \"human resource\", \"salary\", \"payroll\", \"performance\", \"rating\", \"leave\", \"attendance\", \"hiring\", \"recruitment\", \"onboarding\"],\n",
    "    \"engineering\": [\n",
    "        \"engineering\", \"technical\", \"technology\", \"tech\", \"tech stack\", \"tech-stack\", \"stack\", \"architecture\",\n",
    "        \"development\", \"devops\", \"infrastructure\", \"api\", \"apis\", \"microservice\", \"microservices\",\n",
    "        \"deployment\", \"security\", \"framework\", \"frameworks\", \"platform\", \"platforms\", \"language\", \"languages\",\n",
    "        \"tooling\"\n",
    "    ],\n",
    "    \"general\": [\"policy\", \"handbook\", \"guideline\", \"benefit\", \"office\", \"company\", \"event\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3365e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryProcessor:\n",
    "    \"\"\"Process and enhance queries using spaCy NLP\"\"\"\n",
    "    \n",
    "    def __init__(self, nlp_model):\n",
    "        self.nlp = nlp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6422c94",
   "metadata": {},
   "source": [
    "## Query Processor Class Setup\n",
    "\n",
    "We'll build the QueryProcessor class step by step by adding methods to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "19a30ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query sanitization method\n",
    "def sanitize_query(self, query: str) -> str:\n",
    "    \"\"\"Clean and normalize the query\"\"\"\n",
    "    # Remove extra whitespace\n",
    "    query = \" \".join(query.split())\n",
    "    \n",
    "    # Remove special characters except alphanumeric and common punctuation\n",
    "    import re\n",
    "    query = re.sub(r'[^\\w\\s\\?\\.\\,\\-]', '', query)\n",
    "    \n",
    "    return query.strip()\n",
    "\n",
    "QueryProcessor.sanitize_query = sanitize_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8e8ac80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity extraction method\n",
    "def extract_entities(self, query: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"Extract named entities from query\"\"\"\n",
    "    doc = self.nlp(query)\n",
    "    entities = {\n",
    "        \"persons\": [],\n",
    "        \"orgs\": [],\n",
    "        \"dates\": [],\n",
    "        \"money\": [],\n",
    "        \"numbers\": [],\n",
    "        \"locations\": []\n",
    "    }\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            entities[\"persons\"].append(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            entities[\"orgs\"].append(ent.text)\n",
    "        elif ent.label_ == \"DATE\":\n",
    "            entities[\"dates\"].append(ent.text)\n",
    "        elif ent.label_ == \"MONEY\":\n",
    "            entities[\"money\"].append(ent.text)\n",
    "        elif ent.label_ in [\"CARDINAL\", \"QUANTITY\"]:\n",
    "            entities[\"numbers\"].append(ent.text)\n",
    "        elif ent.label_ in [\"GPE\", \"LOC\"]:\n",
    "            entities[\"locations\"].append(ent.text)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "QueryProcessor.extract_entities = extract_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e8b977c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query lemmatization method\n",
    "def lemmatize_query(self, query: str) -> str:\n",
    "    \"\"\"Lemmatize query for better matching\"\"\"\n",
    "    doc = self.nlp(query)\n",
    "    lemmatized = \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
    "    return lemmatized\n",
    "\n",
    "QueryProcessor.lemmatize_query = lemmatize_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6cc17021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intent detection method\n",
    "def detect_intent(self, query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Detect query intent and target department\"\"\"\n",
    "    doc = self.nlp(query.lower())\n",
    "    \n",
    "    intent = {\n",
    "        \"query_type\": \"unknown\",\n",
    "        \"target_departments\": [],\n",
    "        \"is_comparison\": False,\n",
    "        \"is_aggregation\": False,\n",
    "        \"temporal_scope\": None,\n",
    "        \"confidence\": 0.0\n",
    "    }\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # Detect department based on keywords\n",
    "    dept_scores = {}\n",
    "    for dept, keywords in DEPARTMENT_KEYWORDS.items():\n",
    "        score = sum(1 for keyword in keywords if keyword in query_lower)\n",
    "        if score > 0:\n",
    "            dept_scores[dept] = score\n",
    "    \n",
    "    if dept_scores:\n",
    "        max_score = max(dept_scores.values())\n",
    "        intent[\"target_departments\"] = [dept for dept, score in dept_scores.items() if score >= max_score * 0.7]\n",
    "        intent[\"confidence\"] = min(max_score / 5, 1.0)\n",
    "    \n",
    "    # Explicit boost for technology-related queries\n",
    "    tech_terms = [\n",
    "        \"tech\", \"technology\", \"tech stack\", \"tech-stack\", \"stack\", \"framework\", \"frameworks\",\n",
    "        \"programming language\", \"languages\", \"platform\", \"platforms\", \"tooling\"\n",
    "    ]\n",
    "    if any(term in query_lower for term in tech_terms):\n",
    "        if \"engineering\" not in intent[\"target_departments\"]:\n",
    "            intent[\"target_departments\"].append(\"engineering\")\n",
    "        intent[\"confidence\"] = max(intent[\"confidence\"], 0.7)\n",
    "        if intent[\"query_type\"] == \"unknown\":\n",
    "            intent[\"query_type\"] = \"document_search\"\n",
    "    \n",
    "    # Detect query type\n",
    "    if any(word in query_lower for word in [\"employee\", \"salary\", \"payroll\", \"attendance\", \"performance rating\"]):\n",
    "        intent[\"query_type\"] = \"hr_data\"\n",
    "    elif any(word in query_lower for word in [\"policy\", \"handbook\", \"guideline\", \"procedure\"]):\n",
    "        intent[\"query_type\"] = \"document_search\"\n",
    "    elif any(word in query_lower for word in [\"compare\", \"versus\", \"vs\", \"difference between\"]):\n",
    "        intent[\"is_comparison\"] = True\n",
    "        intent[\"query_type\"] = \"comparison\"\n",
    "    elif any(word in query_lower for word in [\"total\", \"sum\", \"average\", \"count\", \"how many\"]):\n",
    "        intent[\"is_aggregation\"] = True\n",
    "    \n",
    "    # Detect temporal scope\n",
    "    if any(word in query_lower for word in [\"q1\", \"q2\", \"q3\", \"q4\", \"quarter\"]):\n",
    "        intent[\"temporal_scope\"] = \"quarterly\"\n",
    "    elif any(word in query_lower for word in [\"2024\", \"2025\", \"year\", \"annual\"]):\n",
    "        intent[\"temporal_scope\"] = \"annual\"\n",
    "    \n",
    "    # Default to document search if we identified departments but no explicit type\n",
    "    if intent[\"query_type\"] == \"unknown\" and intent[\"target_departments\"]:\n",
    "        intent[\"query_type\"] = \"document_search\"\n",
    "    \n",
    "    # Ensure departments are unique and ordered\n",
    "    intent[\"target_departments\"] = list(dict.fromkeys(intent[\"target_departments\"]))\n",
    "    \n",
    "    return intent\n",
    "\n",
    "QueryProcessor.detect_intent = detect_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c882b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query expansion method\n",
    "def expand_query(self, query: str) -> List[str]:\n",
    "    \"\"\"Generate query variations for better retrieval\"\"\"\n",
    "    doc = self.nlp(query)\n",
    "    \n",
    "    # Original query\n",
    "    queries = [query]\n",
    "    \n",
    "    # Lemmatized version\n",
    "    lemmatized = \" \".join([token.lemma_ for token in doc if not token.is_punct])\n",
    "    if lemmatized != query:\n",
    "        queries.append(lemmatized)\n",
    "    \n",
    "    # Extract key noun phrases\n",
    "    noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "    if noun_phrases:\n",
    "        queries.append(\" \".join(noun_phrases))\n",
    "    \n",
    "    return queries[:3]  # Limit to 3 variations\n",
    "\n",
    "QueryProcessor.expand_query = expand_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "67ee6af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete query processing pipeline\n",
    "def process_query(self, query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Complete query processing pipeline\"\"\"\n",
    "    # Step 1: Sanitize\n",
    "    clean_query = self.sanitize_query(query)\n",
    "    \n",
    "    # Step 2: Extract entities\n",
    "    entities = self.extract_entities(clean_query)\n",
    "    \n",
    "    # Step 3: Detect intent\n",
    "    intent = self.detect_intent(clean_query)\n",
    "    \n",
    "    # Step 4: Lemmatize for search\n",
    "    lemmatized = self.lemmatize_query(clean_query)\n",
    "    \n",
    "    # Step 5: Generate query variations\n",
    "    query_variations = self.expand_query(clean_query)\n",
    "    \n",
    "    return {\n",
    "        \"original_query\": query,\n",
    "        \"clean_query\": clean_query,\n",
    "        \"lemmatized_query\": lemmatized,\n",
    "        \"query_variations\": query_variations,\n",
    "        \"entities\": entities,\n",
    "        \"intent\": intent\n",
    "    }\n",
    "\n",
    "QueryProcessor.process_query = process_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "494d2350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ QueryProcessor initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "query_processor = QueryProcessor(nlp)\n",
    "print(\"✅ QueryProcessor initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e2673b",
   "metadata": {},
   "source": [
    "## Testing and Query Execution\n",
    "\n",
    "Now we can test the complete system with different user roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5961df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_token(token: str) -> Dict[str, Any]:\n",
    "    \"\"\"Verify JWT token and return payload\"\"\"\n",
    "    try:\n",
    "        payload = jwt.decode(token, os.getenv(\"JWT_SECRET_KEY\"), algorithms=['HS256'])\n",
    "        return payload\n",
    "    except jwt.ExpiredSignatureError:\n",
    "        raise Exception(\"Token has expired\")\n",
    "    except jwt.InvalidTokenError:\n",
    "        raise Exception(\"Invalid token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "362da85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_rag_response(question: str, user_role: str, processed_query: Dict) -> Generator[str, None, None]:\n",
    "    \"\"\"Generate streaming markdown response using processed query\"\"\"\n",
    "    \n",
    "    allowed_depts = ROLE_PERMISSIONS.get(user_role, [\"general\"])\n",
    "    intent = processed_query[\"intent\"]\n",
    "    entities = processed_query[\"entities\"]\n",
    "    \n",
    "    # Display query analysis\n",
    "    yield \"## 🔍 Query Analysis\\n\\n\"\n",
    "    yield f\"**Original Query:** {processed_query['original_query']}\\n\\n\"\n",
    "    \n",
    "    if intent[\"target_departments\"]:\n",
    "        yield f\"**Detected Departments:** {', '.join(intent['target_departments'])}\\n\\n\"\n",
    "    \n",
    "    if intent[\"query_type\"] != \"unknown\":\n",
    "        yield f\"**Query Type:** {intent['query_type'].replace('_', ' ').title()}\\n\\n\"\n",
    "    \n",
    "    if entities[\"dates\"]:\n",
    "        yield f\"**Time Period:** {', '.join(entities['dates'])}\\n\\n\"\n",
    "    \n",
    "    yield \"---\\n\\n\"\n",
    "    \n",
    "    # HR data query handling\n",
    "    if intent[\"query_type\"] == \"hr_data\":\n",
    "        if \"hr\" in allowed_depts:\n",
    "            yield \"🔍 **Querying HR Database...**\\n\\n\"\n",
    "            status, hr_df = query_hr_data_extended(question, user_role, processed_query)\n",
    "            yield f\"{status}\\n\\n\"\n",
    "            \n",
    "            if hr_df is not None:\n",
    "                yield \"### 📊 HR Data Results\\n\\n\"\n",
    "                yield f\"```\\n{hr_df.to_string(index=False)}\\n```\\n\\n\"\n",
    "                \n",
    "                # Save to CSV\n",
    "                output_file = f\"hr_query_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "                hr_df.to_csv(output_file, index=False)\n",
    "                yield f\"✅ **Data exported to:** `{output_file}`\\n\\n\"\n",
    "            return\n",
    "        else:\n",
    "            yield \"❌ **Access Denied:** You don't have permission to access HR data.\\n\\n\"\n",
    "            return\n",
    "    \n",
    "    # Continue with document-based RAG (next cells will handle this)\n",
    "    yield \"🔍 **Searching documents...**\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e59f28d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document retrieval part of RAG\n",
    "def retrieve_documents(processed_query: Dict, allowed_depts: List[str]) -> Tuple[List, List[str], bool]:\n",
    "    \"\"\"Retrieve and deduplicate documents based on query\"\"\"\n",
    "    intent = processed_query[\"intent\"]\n",
    "    used_fallback = False\n",
    "    \n",
    "    # Determine search scope based on detected departments and permissions\n",
    "    search_depts = intent[\"target_departments\"] if intent[\"target_departments\"] else allowed_depts\n",
    "    search_depts = [d for d in search_depts if d in allowed_depts]\n",
    "    \n",
    "    # Ensure we have at least the allowed departments to search\n",
    "    if not search_depts:\n",
    "        search_depts = allowed_depts\n",
    "    \n",
    "    query_variations = processed_query.get(\"query_variations\") or [processed_query.get(\"clean_query\", \"\")]\n",
    "    query_variations = [q for q in query_variations if q]\n",
    "    \n",
    "    def run_retrieval(departments: Optional[List[str]]) -> List:\n",
    "        search_kwargs = {\"k\": 4}\n",
    "        if departments:\n",
    "            search_kwargs[\"filter\"] = {\"department\": {\"$in\": departments}}\n",
    "        docs = []\n",
    "        for query_var in query_variations:\n",
    "            retriever = vector_store.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs=search_kwargs\n",
    "            )\n",
    "            docs.extend(retriever.invoke(query_var))\n",
    "        return docs\n",
    "    \n",
    "    def deduplicate(docs: List) -> List:\n",
    "        seen_sources = set()\n",
    "        unique = []\n",
    "        for doc in docs:\n",
    "            metadata = doc.metadata or {}\n",
    "            source = metadata.get(\"source\") or metadata.get(\"file_path\") or metadata.get(\"path\")\n",
    "            if not source:\n",
    "                source = metadata.get(\"document_id\") or metadata.get(\"id\")\n",
    "            if not source:\n",
    "                source = doc.page_content[:120]\n",
    "            if source in seen_sources:\n",
    "                continue\n",
    "            seen_sources.add(source)\n",
    "            unique.append(doc)\n",
    "        return unique\n",
    "    \n",
    "    effective_depts = search_depts\n",
    "    unique_docs = deduplicate(run_retrieval(search_depts))\n",
    "    \n",
    "    # First fallback: broaden to all allowed departments\n",
    "    if not unique_docs and allowed_depts:\n",
    "        used_fallback = True\n",
    "        effective_depts = allowed_depts\n",
    "        unique_docs = deduplicate(run_retrieval(allowed_depts))\n",
    "    \n",
    "    # Final fallback: drop department filter entirely\n",
    "    if not unique_docs:\n",
    "        used_fallback = True\n",
    "        effective_depts = allowed_depts\n",
    "        unique_docs = deduplicate(run_retrieval(None))\n",
    "    \n",
    "    return unique_docs, effective_depts, used_fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ef1e1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM response generation part\n",
    "def generate_llm_response(processed_query: Dict, context: str, user_role: str, search_depts: List[str]) -> Generator[str, None, None]:\n",
    "    \"\"\"Generate streaming LLM response\"\"\"\n",
    "    entities = processed_query[\"entities\"]\n",
    "    intent = processed_query[\"intent\"]\n",
    "    \n",
    "    # Create enhanced prompt with spaCy insights\n",
    "    prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant for FinSolve Technologies.\n",
    "\n",
    "**User Role:** {role}\n",
    "**Accessible Departments:** {departments}\n",
    "**Query Type:** {query_type}\n",
    "**Detected Entities:** {entities}\n",
    "\n",
    "**Context from company documents:**\n",
    "{context}\n",
    "\n",
    "**Question:** {question}\n",
    "\n",
    "**Instructions:**\n",
    "1. Answer based ONLY on the provided context\n",
    "2. Format response in clear markdown with headers, bullet points, and emphasis\n",
    "3. Cite specific sources by document name\n",
    "4. If comparing data, use tables or structured format\n",
    "5. If information is missing, clearly state that\n",
    "\n",
    "**Answer:**\n",
    "\"\"\")\n",
    "    \n",
    "    # Initialize LLM with streaming\n",
    "    llm = ChatGroq(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        temperature=0.3,\n",
    "        streaming=True,\n",
    "        groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    entities_str = \", \".join([f\"{k}: {v}\" for k, v in entities.items() if v])\n",
    "    \n",
    "    prompt = prompt_template.format(\n",
    "        role=user_role,\n",
    "        departments=\", \".join(search_depts),\n",
    "        query_type=intent[\"query_type\"],\n",
    "        entities=entities_str if entities_str else \"None\",\n",
    "        context=context,\n",
    "        question=processed_query[\"clean_query\"]\n",
    "    )\n",
    "    \n",
    "    # Stream response\n",
    "    for chunk in llm.stream(prompt):\n",
    "        if hasattr(chunk, 'content'):\n",
    "            yield chunk.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "25863f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete stream_rag_response - Document flow continuation\n",
    "def stream_rag_response_documents(question: str, user_role: str, processed_query: Dict) -> Generator[str, None, None]:\n",
    "    \"\"\"Document-based RAG streaming response\"\"\"\n",
    "    allowed_depts = ROLE_PERMISSIONS.get(user_role, [\"general\"])\n",
    "    \n",
    "    # Retrieve documents\n",
    "    unique_docs, search_depts, used_fallback = retrieve_documents(processed_query, allowed_depts)\n",
    "    \n",
    "    if not allowed_depts and not search_depts:\n",
    "        yield \"⚠️ **No accessible departments found for this user.**\\n\\n\"\n",
    "        return\n",
    "    \n",
    "    search_depts_display = list(dict.fromkeys(search_depts)) if search_depts else []\n",
    "    if search_depts_display:\n",
    "        yield f\"**Searching in:** {', '.join(search_depts_display)}\\n\\n\"\n",
    "    else:\n",
    "        yield \"ℹ️ **Searching across all accessible documents.**\\n\\n\"\n",
    "    \n",
    "    if used_fallback:\n",
    "        yield \"ℹ️ **Expanded search scope due to limited initial matches.**\\n\\n\"\n",
    "    \n",
    "    if not unique_docs:\n",
    "        yield \"⚠️ **No relevant information found in accessible documents.**\\n\\n\"\n",
    "        return\n",
    "    \n",
    "    yield f\"📄 **Found {len(unique_docs)} relevant documents**\\n\\n\"\n",
    "    \n",
    "    # Prepare context\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Source: {doc.metadata.get('source', 'Unknown')}\\nDepartment: {doc.metadata.get('department', 'Unknown')}\\n{doc.page_content}\"\n",
    "        for doc in unique_docs[:5]\n",
    "    ])\n",
    "    \n",
    "    yield \"💬 **Generating response...**\\n\\n---\\n\\n\"\n",
    "    \n",
    "    # Generate LLM response\n",
    "    for chunk in generate_llm_response(processed_query, context, user_role, search_depts_display or allowed_depts):\n",
    "        yield chunk\n",
    "    \n",
    "    # Add sources\n",
    "    yield \"\\n\\n---\\n\\n### 📚 Sources\\n\\n\"\n",
    "    for i, doc in enumerate(unique_docs[:5], 1):\n",
    "        source = doc.metadata.get('source', 'Unknown').split('/')[-1]\n",
    "        dept = doc.metadata.get('department', 'Unknown')\n",
    "        yield f\"{i}. **{source}** (Department: {dept})\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f0689d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update stream_rag_response to use the document flow\n",
    "def stream_rag_response_complete(question: str, user_role: str, processed_query: Dict) -> Generator[str, None, None]:\n",
    "    \"\"\"Complete streaming RAG response - combines HR and document flows\"\"\"\n",
    "    \n",
    "    # First yield the analysis part (from stream_rag_response)\n",
    "    for chunk in stream_rag_response(question, user_role, processed_query):\n",
    "        yield chunk\n",
    "    \n",
    "    # If it's not HR query, continue with document search\n",
    "    intent = processed_query[\"intent\"]\n",
    "    if intent[\"query_type\"] != \"hr_data\":\n",
    "        for chunk in stream_rag_response_documents(question, user_role, processed_query):\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9a2312a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_query(question: str, token: str):\n",
    "    \"\"\"Main function with spaCy-enhanced query processing\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Verify token\n",
    "        payload = verify_token(token)\n",
    "        user_role = payload.get('role')\n",
    "        user_id = payload.get('id')\n",
    "        \n",
    "        print(f\"👤 **User:** {user_id} | **Role:** {user_role}\")\n",
    "        print(f\"🔑 **Access:** {', '.join(ROLE_PERMISSIONS.get(user_role, ['general']))}\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        print()\n",
    "        \n",
    "        # Process query with spaCy\n",
    "        print(\"🧠 **Processing query with spaCy...**\\n\")\n",
    "        processed_query = query_processor.process_query(question)\n",
    "        \n",
    "        # Stream response using complete flow\n",
    "        for chunk in stream_rag_response_complete(question, user_role, processed_query):\n",
    "            print(chunk, end='', flush=True)\n",
    "        \n",
    "        print(\"\\n\\n\" + \"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ **Error:** {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8c770a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Testing spaCy Query Processor\n",
      "\n",
      "**Original:** What was our Q3 2024 marketing spend and campaign performance?\n",
      "**Clean:** What was our Q3 2024 marketing spend and campaign performance?\n",
      "**Lemmatized:** q3 2024 marketing spend campaign performance ?\n",
      "**Variations:** ['What was our Q3 2024 marketing spend and campaign performance?', 'what be our q3 2024 marketing spend and campaign performance', 'What our Q3 2024 marketing spend campaign performance']\n",
      "**Entities:** {'persons': [], 'orgs': [], 'dates': ['Q3 2024'], 'money': [], 'numbers': [], 'locations': []}\n",
      "**Intent:** {'query_type': 'document_search', 'target_departments': ['marketing'], 'is_comparison': False, 'is_aggregation': False, 'temporal_scope': 'quarterly', 'confidence': 0.4}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n### Testing spaCy Query Processor\\n\")\n",
    "\n",
    "test_query = \"What was our Q3 2024 marketing spend and campaign performance?\"\n",
    "processed = query_processor.process_query(test_query)\n",
    "\n",
    "print(f\"**Original:** {processed['original_query']}\")\n",
    "print(f\"**Clean:** {processed['clean_query']}\")\n",
    "print(f\"**Lemmatized:** {processed['lemmatized_query']}\")\n",
    "print(f\"**Variations:** {processed['query_variations']}\")\n",
    "print(f\"**Entities:** {processed['entities']}\")\n",
    "print(f\"**Intent:** {processed['intent']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "32460062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Query 1: Marketing Data\n",
      "\n",
      "👤 **User:** 1 | **Role:** Manager\n",
      "🔑 **Access:** finance, hr, engineering, marketing, general\n",
      "\n",
      "================================================================================\n",
      "\n",
      "🧠 **Processing query with spaCy...**\n",
      "\n",
      "## 🔍 Query Analysis\n",
      "\n",
      "**Original Query:** What were the marketing strategies and ROI in Q3 2024?\n",
      "\n",
      "**Detected Departments:** marketing\n",
      "\n",
      "**Query Type:** Document Search\n",
      "\n",
      "**Time Period:** Q3 2024\n",
      "\n",
      "---\n",
      "\n",
      "🔍 **Searching documents...**\n",
      "\n",
      "**Original Query:** What were the marketing strategies and ROI in Q3 2024?\n",
      "\n",
      "**Detected Departments:** marketing\n",
      "\n",
      "**Query Type:** Document Search\n",
      "\n",
      "**Time Period:** Q3 2024\n",
      "\n",
      "---\n",
      "\n",
      "🔍 **Searching documents...**\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Searching in:** marketing\n",
      "\n",
      "📄 **Found 3 relevant documents**\n",
      "\n",
      "💬 **Generating response...**\n",
      "\n",
      "---\n",
      "\n",
      "📄 **Found 3 relevant documents**\n",
      "\n",
      "💬 **Generating response...**\n",
      "\n",
      "---\n",
      "\n",
      "**Marketing Strategies in Q3 2024**\n",
      "=====================================\n",
      "\n",
      "According to the Comprehensive Marketing Report - Q3 2024 [1], the marketing strategy in Q3 2024 centered on driving revenue growth through increased transaction volume from loyalty programs and new market sales.\n",
      "\n",
      "**Key Initiatives:**\n",
      "* Revenue Target: $7.5 million\n",
      "* Marketing Spend: $2 million, allocated across:\n",
      "\t+ Digital advertising (35%)\n",
      "\t+ Loyalty programs (20%)\n",
      "\t+ Event sponsorships (20%)\n",
      "\t+ Merchant partnerships (15%)\n",
      "\t+ Content marketing (10%)\n",
      "\n",
      "**Performance Benchmarks:**\n",
      "-------------------------\n",
      "\n",
      "The following performance benchmarks were established to evaluate the success of marketing initiatives in Q3 2024:\n",
      "\n",
      "| Benchmark | Target |\n",
      "| --- | --- |\n",
      "| Conversion Rate | 14.0% |\n",
      "| ROI | 3.75x |\n",
      "| Customer Retention Rate | 82% |\n",
      "| Cost Per Acquisition (CPA) | $11.11 |\n",
      "\n",
      "**ROI in Q3 2024**\n",
      "------------------\n",
      "\n",
      "The ROI target for Q3 2024 was 3.75x, measured as the ratio of incremental revenue from marketing activities to the total marketing spend.\n",
      "\n",
      "**References:**\n",
      "[1] Comprehensive Marketing Report - Q3 2024 (data/marketing/marketing_report_q3_2024.md)\n",
      "\n",
      "---\n",
      "\n",
      "### 📚 Sources\n",
      "\n",
      "1. **market_report_q4_2024.md** (Department: marketing)\n",
      "2. **marketing_report_q2_2024.md** (Department: marketing)\n",
      "3. **marketing_report_q3_2024.md** (Department: marketing)\n",
      "\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Query 1: Document search with entity extraction\n",
    "print(\"\\n\\n### Query 1: Marketing Data\\n\")\n",
    "handle_query(\n",
    "    \"What were the marketing strategies and ROI in Q3 2024?\",\n",
    "    found_item['token']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3860e4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Query 2: HR Data with Entity Recognition\n",
      "\n",
      "👤 **User:** 1 | **Role:** Manager\n",
      "🔑 **Access:** finance, hr, engineering, marketing, general\n",
      "\n",
      "================================================================================\n",
      "\n",
      "🧠 **Processing query with spaCy...**\n",
      "\n",
      "## 🔍 Query Analysis\n",
      "\n",
      "**Original Query:** Show me the top 5 highest paid employees in the Finance department\n",
      "\n",
      "**Detected Departments:** finance, hr\n",
      "\n",
      "**Query Type:** Hr Data\n",
      "\n",
      "---\n",
      "\n",
      "🔍 **Querying HR Database...**\n",
      "\n",
      "✅ HR Data Retrieved\n",
      "\n",
      "### 📊 HR Data Results\n",
      "\n",
      "```\n",
      "employee_id         full_name        department                 role\n",
      " FINEMP1000      Aadhya Patel             Sales        Sales Manager\n",
      " FINEMP1001    Isha Chowdhury           Finance       Credit Officer\n",
      " FINEMP1002   Sakshi Malhotra             Sales Relationship Manager\n",
      " FINEMP1003  Krishna Malhotra          Business     Business Analyst\n",
      " FINEMP1004     Aadhya Saxena         Marketing    Marketing Manager\n",
      " FINEMP1005     Shaurya Joshi           Finance    Financial Analyst\n",
      " FINEMP1006       Sara Sharma Quality Assurance          QA Engineer\n",
      " FINEMP1007      Prisha Mehta         Marketing    Marketing Manager\n",
      " FINEMP1008  Aadhya Chowdhury        Operations     Customer Support\n",
      " FINEMP1009         Sai Gupta           Finance       Credit Officer\n",
      " FINEMP1010  Vihaan Chowdhury                HR           HR Manager\n",
      " FINEMP1011        Sai Sharma          Business     Business Analyst\n",
      " FINEMP1012      Ishaan Patel        Technology    Security Engineer\n",
      " FINEMP1013   Prisha Banerjee        Technology    Security Engineer\n",
      " FINEMP1014        Isha Desai           Finance     Treasury Analyst\n",
      " FINEMP1015      Ananya Singh                HR           HR Manager\n",
      " FINEMP1016      Ananya Reddy             Sales Relationship Manager\n",
      " FINEMP1017     Prisha Saxena        Technology      DevOps Engineer\n",
      " FINEMP1018      Vivaan Reddy        Compliance   Compliance Officer\n",
      " FINEMP1019    Isha Chowdhury             Sales Relationship Manager\n",
      " FINEMP1020     Krishna Verma             Sales        Sales Manager\n",
      " FINEMP1021      Vihaan Verma           Finance     Treasury Analyst\n",
      " FINEMP1022          Sai Khan        Operations     Customer Support\n",
      " FINEMP1023     Krishna Gupta          Business     Business Analyst\n",
      " FINEMP1024     Reyansh Mehta        Technology    Security Engineer\n",
      " FINEMP1025     Krishna Reddy         Marketing    Marketing Manager\n",
      " FINEMP1026      Ishaan Patel        Technology Blockchain Developer\n",
      " FINEMP1027        Myra Desai        Technology    Security Engineer\n",
      " FINEMP1028         Isha Nair        Compliance   Compliance Officer\n",
      " FINEMP1029      Aditya Patel              Data         Data Analyst\n",
      " FINEMP1030      Vihaan Reddy              Data       Data Scientist\n",
      " FINEMP1031        Isha Desai        Compliance   Compliance Officer\n",
      " FINEMP1032       Arjun Desai           Finance    Financial Analyst\n",
      " FINEMP1033       Saanvi Bhat              Risk         Risk Analyst\n",
      " FINEMP1034        Diya Desai         Marketing    Marketing Manager\n",
      " FINEMP1035      Arjun Chopra           Finance     Treasury Analyst\n",
      " FINEMP1036         Avni Khan          Business     Business Analyst\n",
      " FINEMP1037       Vihaan Garg             Sales        Sales Manager\n",
      " FINEMP1038         Diya Bhat        Technology    Security Engineer\n",
      " FINEMP1039         Sai Desai        Technology      DevOps Engineer\n",
      " FINEMP1040      Vihaan Reddy Quality Assurance          QA Engineer\n",
      " FINEMP1041     Shaurya Joshi             Sales Relationship Manager\n",
      " FINEMP1042      Vihaan Desai        Operations     Customer Support\n",
      " FINEMP1043      Aadhya Singh             Sales        Sales Manager\n",
      " FINEMP1044        Isha Desai        Technology    Software Engineer\n",
      " FINEMP1045   Ananya Banerjee              Data         Data Analyst\n",
      " FINEMP1046     Shaurya Joshi         Marketing    Marketing Manager\n",
      " FINEMP1047      Ishaan Singh          Business     Business Analyst\n",
      " FINEMP1048        Isha Singh           Finance    Financial Analyst\n",
      " FINEMP1049        Arjun Garg           Product      Product Manager\n",
      " FINEMP1050        Myra Gupta Quality Assurance          QA Engineer\n",
      " FINEMP1051    Reyansh Saxena        Technology    Security Engineer\n",
      " FINEMP1052      Ishaan Singh             Sales Relationship Manager\n",
      " FINEMP1053       Isha Sharma        Operations     Customer Support\n",
      " FINEMP1054      Ishaan Singh              Data       Data Scientist\n",
      " FINEMP1055     Aditya Saxena                HR           HR Manager\n",
      " FINEMP1056       Saanvi Bhat            Design          UX Designer\n",
      " FINEMP1057     Aditya Kapoor         Marketing    Marketing Manager\n",
      " FINEMP1058       Avni Chopra           Finance    Financial Analyst\n",
      " FINEMP1059   Saanvi Malhotra        Operations     Customer Support\n",
      " FINEMP1060       Arjun Mehta             Sales        Sales Manager\n",
      " FINEMP1061  Vivaan Chowdhury        Technology Blockchain Developer\n",
      " FINEMP1062       Ananya Iyer           Product      Product Manager\n",
      " FINEMP1063     Aadhya Kapoor              Risk         Risk Analyst\n",
      " FINEMP1064       Saanvi Nair              Risk         Risk Analyst\n",
      " FINEMP1065         Myra Garg        Technology Blockchain Developer\n",
      " FINEMP1066         Diya Iyer             Sales        Sales Manager\n",
      " FINEMP1067    Shaurya Chopra        Compliance   Compliance Officer\n",
      " FINEMP1068     Shaurya Singh              Data       Data Scientist\n",
      " FINEMP1069     Vivaan Saxena              Risk         Risk Analyst\n",
      " FINEMP1070   Saanvi Banerjee           Finance       Credit Officer\n",
      " FINEMP1071      Vivaan Verma           Finance       Credit Officer\n",
      " FINEMP1072      Saanvi Gupta Quality Assurance          QA Engineer\n",
      " FINEMP1073         Diya Nair             Sales Relationship Manager\n",
      " FINEMP1074     Vihaan Chopra             Sales Relationship Manager\n",
      " FINEMP1075       Ananya Khan Quality Assurance          QA Engineer\n",
      " FINEMP1076    Avni Chowdhury           Finance       Credit Officer\n",
      " FINEMP1077      Krishna Nair            Design          UX Designer\n",
      " FINEMP1078     Prisha Chopra              Data         Data Analyst\n",
      " FINEMP1079      Krishna Iyer           Finance    Financial Analyst\n",
      " FINEMP1080        Avni Reddy              Data         Data Analyst\n",
      " FINEMP1081   Sakshi Malhotra           Finance       Credit Officer\n",
      " FINEMP1082  Vihaan Chowdhury           Finance    Financial Analyst\n",
      " FINEMP1083  Krishna Malhotra             Sales        Sales Manager\n",
      " FINEMP1084  Saanvi Chowdhury          Business     Business Analyst\n",
      " FINEMP1085      Aadhya Mehta Quality Assurance          QA Engineer\n",
      " FINEMP1086         Diya Iyer              Data         Data Analyst\n",
      " FINEMP1087         Avni Nair             Sales Relationship Manager\n",
      " FINEMP1088    Avni Chowdhury        Operations     Customer Support\n",
      " FINEMP1089       Aarav Joshi        Technology    Security Engineer\n",
      " FINEMP1090   Arjun Chowdhury        Technology    Software Engineer\n",
      " FINEMP1091        Diya Desai        Compliance   Compliance Officer\n",
      " FINEMP1092       Ananya Nair                HR           HR Manager\n",
      " FINEMP1093    Shaurya Sharma           Product      Product Manager\n",
      " FINEMP1094      Aarav Kapoor            Design          UX Designer\n",
      " FINEMP1095       Ananya Khan Quality Assurance          QA Engineer\n",
      " FINEMP1096      Krishna Bhat           Finance       Credit Officer\n",
      " FINEMP1097     Sakshi Kapoor         Marketing    Marketing Manager\n",
      " FINEMP1098       Arjun Patel              Risk         Risk Analyst\n",
      " FINEMP1099 Shaurya Chowdhury        Technology      DevOps Engineer\n",
      "```\n",
      "\n",
      "✅ **Data exported to:** `hr_query_20251026_114651.csv`\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Query 2: HR Data with Entity Recognition (if user has HR access)\n",
    "if \"hr\" in ROLE_PERMISSIONS.get(found_item['role'], []):\n",
    "    print(\"\\n\\n### Query 2: HR Data with Entity Recognition\\n\")\n",
    "    handle_query(\n",
    "        \"Show me the top 5 highest paid employees in the Finance department\",\n",
    "        found_item['token']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ad6a9290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Query 3: Complex Multi-Department Query\n",
      "\n",
      "👤 **User:** 1 | **Role:** Manager\n",
      "🔑 **Access:** finance, hr, engineering, marketing, general\n",
      "\n",
      "================================================================================\n",
      "\n",
      "🧠 **Processing query with spaCy...**\n",
      "\n",
      "## 🔍 Query Analysis\n",
      "\n",
      "**Original Query:** Compare our Q3 and Q4 marketing expenses and tell me which quarter performed better\n",
      "\n",
      "**Detected Departments:** finance\n",
      "\n",
      "**Query Type:** Comparison\n",
      "\n",
      "---\n",
      "\n",
      "🔍 **Searching documents...**\n",
      "\n",
      "**Searching in:** finance\n",
      "\n",
      "📄 **Found 1 relevant documents**\n",
      "\n",
      "💬 **Generating response...**\n",
      "\n",
      "---\n",
      "\n",
      "**Marketing Expense Comparison between Q3 and Q4**\n",
      "=====================================================\n",
      "\n",
      "**Source:** data/finance/quarterly_financial_report.md\n",
      "\n",
      "Unfortunately, the provided context does not contain information about Q3 marketing expenses. However, we can compare the marketing expenses for Q4, which is the only quarter with available data.\n",
      "\n",
      "**Q4 Marketing Expenses**\n",
      "-------------------------\n",
      "\n",
      "* Marketing Spend: $500 million (Source: data/finance/quarterly_financial_report.md)\n",
      "* Vendor Costs: $120 million (Source: data/finance/quarterly_financial_report.md)\n",
      "\n",
      "Since there is no data available for Q3, we cannot perform a direct comparison between the two quarters. However, we can analyze the marketing expenses for Q4.\n",
      "\n",
      "**Conclusion**\n",
      "----------\n",
      "\n",
      "Based on the available data, we can conclude that:\n",
      "\n",
      "* Q4 marketing expenses were $500 million.\n",
      "* Q4 vendor costs were $120 million.\n",
      "\n",
      "Unfortunately, without Q3 data, we cannot determine which quarter performed better in terms of marketing expenses.\n",
      "\n",
      "---\n",
      "\n",
      "### 📚 Sources\n",
      "\n",
      "1. **quarterly_financial_report.md** (Department: finance)\n",
      "\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Query 3: Complex Multi-Department Query\n",
    "print(\"\\n\\n### Query 3: Complex Multi-Department Query\\n\")\n",
    "handle_query(\n",
    "    \"Compare our Q3 and Q4 marketing expenses and tell me which quarter performed better\",\n",
    "    found_item['token']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0bcda9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Query 4: Tech Stack Query\n",
      "\n",
      "👤 **User:** 1 | **Role:** Manager\n",
      "🔑 **Access:** finance, hr, engineering, marketing, general\n",
      "\n",
      "================================================================================\n",
      "\n",
      "🧠 **Processing query with spaCy...**\n",
      "\n",
      "## 🔍 Query Analysis\n",
      "\n",
      "**Original Query:** Tell me about the tech stack of the company\n",
      "\n",
      "**Detected Departments:** engineering\n",
      "\n",
      "**Query Type:** Document Search\n",
      "\n",
      "---\n",
      "\n",
      "🔍 **Searching documents...**\n",
      "\n",
      "**Original Query:** Tell me about the tech stack of the company\n",
      "\n",
      "**Detected Departments:** engineering\n",
      "\n",
      "**Query Type:** Document Search\n",
      "\n",
      "---\n",
      "\n",
      "🔍 **Searching documents...**\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Searching in:** engineering\n",
      "\n",
      "📄 **Found 1 relevant documents**\n",
      "\n",
      "💬 **Generating response...**\n",
      "\n",
      "---\n",
      "\n",
      "📄 **Found 1 relevant documents**\n",
      "\n",
      "💬 **Generating response...**\n",
      "\n",
      "---\n",
      "\n",
      "**Tech Stack Overview**\n",
      "======================\n",
      "\n",
      "According to the engineering master document [1], the tech stack of FinSolve Technologies is outlined below:\n",
      "\n",
      "### System Architecture and Infrastructure\n",
      "\n",
      "* No specific details are provided in the given context about the system architecture and infrastructure.\n",
      "\n",
      "### Software Development Lifecycle (SDLC)\n",
      "\n",
      "* No specific details are provided in the given context about the SDLC.\n",
      "\n",
      "### Technology Stack\n",
      "\n",
      "* Unfortunately, the provided context does not specify the technology stack used by FinSolve Technologies.\n",
      "\n",
      "However, based on the company overview, we can infer that FinSolve Technologies is a FinTech company, which might suggest the use of technologies related to financial services, such as:\n",
      "\n",
      "* Digital banking platforms\n",
      "* Payment processing systems\n",
      "* Wealth management software\n",
      "* Enterprise financial analytics tools\n",
      "\n",
      "Please note that this is an educated guess and not based on specific information from the provided context.\n",
      "\n",
      "**References**\n",
      "--------------\n",
      "\n",
      "[1] data/engineering/engineering_master_doc.md - FinSolve Technologies Engineering Document\n",
      "\n",
      "---\n",
      "\n",
      "### 📚 Sources\n",
      "\n",
      "1. **engineering_master_doc.md** (Department: engineering)\n",
      "\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Query 4: Tech Stack Query (Testing Engineering Department Detection)\n",
    "print(\"\\n\\n### Query 4: Tech Stack Query\\n\")\n",
    "handle_query(\n",
    "    \"Tell me about the tech stack of the company\",\n",
    "    found_item['token']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab2c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
